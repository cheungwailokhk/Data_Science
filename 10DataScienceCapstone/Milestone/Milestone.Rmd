---
title: "Milestone Report"
author: "cwl"
date: "6/5/2021"
output:
  html_document:
    keep_md: yes
  pdf_document:
    latex_engine: xelatex
classoption: landscape
editor_options: 
  chunk_output_type: console
always_allow_html: true
---

1. [Introduction](#introduction)
    a. [Background](#background)
    b. [Objective](#objective)
2. [Dataset](#dataset)
    a. [Source](#source)
    b. [Data information](#information)
3. [Data pre-processing](#preprocessing)
    a. [Data sampling](#sampling)
    b. [Data cleaning](#cleaning)
    c. [N-gram](#ngram)
4. [Exploratory analysis](#exploratory)
    a. [Word frequencies](#frequencies)
    b. [Word Clouds](#wordclouds)
    c. [Clustering](#clustering)
5. [Findings](#findings)
6. [Future plans](#plans)
7. [Appendix A. Top 100 frequent words in N-gram](#appendixA)
8. [Appendix B. Full Code in R](#appendixB)



## Introduction <a name="introduction"></a>
### 1a. Background <a name="background"></a>
This milestone report was a part of the course ([Data Science Capstone](https://www.coursera.org/learn/data-science-project?specialization=jhu-data-science)) offered by Johns Hopkins University on the Coursera platform. This course was under a specialization program, which consisted of 9 courses and a Capstone Project. 

The Capstone Project required students to develop an application to predict the following word after a user typed a phrase. This project started with analyzing a large corpus of text documents to discover the data structure and how words were structured. It covered cleaning and analyzing text data, then building and sampling from a predictive text model. Finally, we applied the knowledge we gained in data products to develop a predictive text product. There were eight tasks in total, and this report aimed to achieve the first four tasks.

- Task 1: **Understanding the Problem**
- Task 2: **Data acquisition and cleaning**
- Task 3: **Exploratory analysis**
- Task 4: **Statistical modeling**
- Task 5: Predictive modeling
- Task 6: Creative exploration
- Task 7: Creating a data product
- Task 8: Creating a short slide deck pitching your product

### 1b. Objective <a name="objective"></a>
The objective of this report was to display the data we were working with, and to understand any basic relationships we were able observe in the data. In this report, we applied several natural language processing and text mining techniques in analyzing any new data.

## 2. Data <a name="dataset"></a>
### 2a. Source <a name="source"></a>
The course ([Data Science Capstone](https://www.coursera.org/learn/data-science-project?specialization=jhu-data-science) provided the training data, and it served as the basis for most of the capstone. The data was in English, German, Russian and Finnish, but we only considered the English one.

**[Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)**

1. en_US.blogs.txt
2. en_US.news.txt
3. en_US.twitter.txt

### 2b. Data basic information <a name="information"></a>
We summarized the basic summary of the dataset, trying to understand their contents. You may preview a few lines of each dataset.

```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Load libraries
require(tm); require(SnowballC); require(stopwords); 
require(ggplot2); require(tokenizers);  require(dplyr);
require(wordcloud2); require(webshot); require(htmlwidgets)

url_tw <- "./data/en_US.twitter.txt"
url_blog <- "./data/en_US.blogs.txt"
url_news <- "./data/en_US.news.txt"
conn_tw <- file(url_tw,open="r")
conn_blog <- file(url_blog,open="r")
conn_news <- file(url_news,open="r")

# Load data
lines_tw <-readLines(conn_tw)
lines_blog <-readLines(conn_blog)
lines_news <-readLines(conn_news)

line_counts <- c(length(lines_tw), length(lines_blog), length(lines_news))
word_counts <- c(sum(nchar(lines_tw)), sum(nchar(lines_blog)),sum(nchar(lines_blog)))
size_counts <- c(format(object.size(lines_tw), units = "Mb"),
                format(object.size(lines_blog), units = "Mb"),
                format(object.size(lines_news), units = "Mb"))

# Process basic infomation
summ <- data.frame('Lines_counts' = line_counts, 
                         'Word_counts' = word_counts, 
                         'File_size' = size_counts,
                      row.names = c('en_US.twitter', 'en_US.blogs', 'en_US.news'))
summ

close(conn_tw)
close(conn_blog)
close(conn_news)
rm(url_tw, url_blog, url_news, conn_blog, conn_news, conn_tw)

loadData <- function (url, sampleRate) {
    # Load data
    conn <- file(url,open="r")
    lines <-readLines(url)  
    close(conn)
    lines
    
    # create a list of random variables
    set.seed(1234)
    lines <- sample(lines, length(lines)*sampleRate, replace=FALSE)
    
    # Print out file information
    print(data.frame('Lines_counts' = length(lines), 
                     'Word_counts' = sum(nchar(lines)), 
                     'File_size' = format(object.size(lines), units = "Mb"),
                     row.names = c(url)))
    lines
}


```
#### Examples of Twitter data:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1234)
# Preview twitter contents
sample(lines_tw, 3)
```

#### Examples of blog data:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1234)
# Preview blog contents
sample(lines_blog,3)
```
#### Examples of news data:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1234)
# Preview news contents
sample(lines_news,3)
```


## 3. Data pre-processing <a name="preprocessing"></a>
### 3a. Data sampling <a name="sampling"></a>
This dataset was fairly large. Our objective was to visualize and understand the outline of dataset, so it was unnecessary to load the entire dataset in to build our algorithms. We sampled a small part of it to do so. 

```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# create a list of random variables
set.seed(1234)
# Sample a part of each dataset
percent <- 0.01
lines_tw <- sample(lines_tw, summ$Lines_counts[1]*percent, replace=FALSE)
lines_blog <- sample(lines_blog, summ$Lines_counts[2]*percent, replace=FALSE)
lines_news <- sample(lines_news, summ$Lines_counts[3]*percent, replace=FALSE)
```

The new summary for samples was:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
line_counts <- c(length(lines_tw), length(lines_blog), length(lines_news))
word_counts <- c(sum(nchar(lines_tw)), sum(nchar(lines_blog)),sum(nchar(lines_blog)))
size_counts <- c(format(object.size(lines_tw), units = "Mb"),
                format(object.size(lines_blog), units = "Mb"),
                format(object.size(lines_news), units = "Mb"))


summ <- data.frame('Lines_counts' = line_counts, 
                         'Word_counts' = word_counts, 
                         'File_size' = size_counts,
                      row.names = c('en_US.twitter', 'en_US.blogs', 'en_US.news'))
summ
rm(line_counts, word_counts, size_counts, percent)
```

### 3b. Data cleaning <a name="cleaning"></a>
As we can obeserve from the samples, there were some unnecessary information such as stopswords, punctuation. In the next step, we performed the following data cleaning to remove these unnecessary information

- Convert to lowercase
- Remove URLs
- Remove punctuation
- remove numbers
- Removing English stopwords (common words)
- Remove special characters
- Stemming (reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words )

```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# 1. Text cleaning:
myCorpus <- VCorpus(VectorSource(c(lines_tw, lines_blog, lines_news)))
rm(lines_tw, lines_blog, lines_news)
removeURL <- function(x) {gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)}
removeSpecial <- function(x) {gsub("[^a-zA-Z ]+", "", x)} # except space
removeEmpty <- function(x){!is.na(content(x)) & trimws(content(x)) != ""}

# Preprocessing
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, content_transformer(removeSpecial))

myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))

# Remove documents with empty content to reduce size
myCorpus <- tm_filter(myCorpus, removeEmpty)

# Stemming
myCorpus <- tm_map(myCorpus, stemDocument)

rm(removeURL, removeSpecial)
print(paste0('Corpus size after cleaning: ', format(object.size(myCorpus), units = "Mb")))
``` 

#### Examples of our corpus after processing:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# inspect the first 5 documents in our corpus  with only stems
for (i in 1:5) {
    cat(paste("[[", i, "]] ", sep = ""))
    writeLines(as.character(myCorpus[[i]]))
}
``` 

### 3c. N-gram <a name="ngram"></a>
After data cleaning, we performed a N-gram preprocessing step to identify appropriate words.

N-gram refered to segmenting an input stream into all combinations of adjacent words of length n in our corpus.

- Unigram: a single letter, syllable, or word.
- Bigram: a pair of consecutive written units such as letters, syllables, or words.
- Trigram: a group of three consecutive written units such as letters, syllables, or words.

Then, we created a term document matrix to get the frequency of our words.

```{r echo = TRUE, message=FALSE, warning=FALSE, cache=TRUE}
# 2. N-gram
# create N-gram tern document matrix from a corpus
createNgram <- function(myCorpus, ngram = 1, lowfreq = NA) {
    toToken <- function(x) {
        # x contains x[1]$content and x[2]$meta
        unlist(tokenize_ngrams(content(x), n = ngram, n_min = ngram))
    }
    
    # TDM automatically discards words less than three characters. 
    tdm <- TermDocumentMatrix(myCorpus, control = list(tokenize = toToken,
                                                       wordLengths=c(1, Inf)))

    if (!is.na(lowfreq)) {
        # remove infrequent words to reduce size
        freqTerms <- findFreqTerms(tdm, lowfreq = lowfreq)
        tdm <- tdm[freqTerms, ]
    }
    tdm
}

convertTdmToDf <- function(tdm) {
    # Reduce documents dimension and convert to dataframe
    # df <- data.frame(rowSums(as.matrix(tdm))) # exhaust memory
    df <- slam::row_sums(tdm, na.rm = T)
    df <- data.frame(word = names(df), freq = as.integer(df))  %>%
        arrange(desc(freq))
    rm(tdm)
    print(paste0("Ngram size: ", format(object.size(df), units = "Mb")))
    df
}

# N-gram
# removeSparseTerms: remove sparse words to contain around 1000 frequent words

# Unigram
tdm1 <- createNgram(myCorpus, n = 1, lowfreq = 1)
freq1 <- convertTdmToDf(tdm1) # Create a frequency sorted dataframe for tokens
# Bigram
tdm2 <- createNgram(myCorpus, n = 2, lowfreq = 1)
freq2 <- convertTdmToDf(tdm2) # Create a frequency sorted dataframe for tokens
# Trigram 
tdm3 <- createNgram(myCorpus, n = 3, lowfreq = 1)
freq3 <- convertTdmToDf(tdm3) # Create a frequency sorted dataframe for tokens
rm(myCorpus)
``` 

#### Examples of Unigram after processing:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Inspect Top 100 frequent words in bigram
set.seed(1234)
sample(freq1$word, 10)
```

#### Examples of Bigram after processing:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Inspect Top 100 frequent words in bigram
set.seed(1234)
sample(freq2$word, 10)
```
#### Examples of Triigram after processing:
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Inspect Top 100 frequent words in Trigram
set.seed(1234)
sample(freq3$word, 10)
```

## 4. Exploratory analysis <a name="exploratory"></a>
We built figures and tables to understand variation in the frequencies of words and word pairs in our data.

### 4a. Word frequencies <a name="frequencies"></a>
This part shows the top 20 frequent terms of our N-gram pre-processing. 

In appendix A, you may find the top 100 frequent words in N-gram.

```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.width= 6.5, fig.height=5}
# Plot Unigram
ggplot(head(freq1,20), aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat = "identity") +
  labs(x = "Terms",
           y = "Count") + 
      ggtitle("Top 20 frequent terms in Unigram") +
  coord_flip()

# Plot Bigram
ggplot(head(freq2,20), aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat = "identity") +
  labs(x = "Terms",
           y = "Count") + 
      ggtitle("Top 20 frequent terms in Bigram") +
  coord_flip()

# Plot Trigram
ggplot(head(freq3,20), aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat = "identity") +
  labs(x = "Terms",
           y = "Count") + 
      ggtitle("Top 20 frequent terms in Trigram") +
  coord_flip()
```

### 4b. Word clouds <a name="wordclouds"></a>

```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Word clouds for Unigram
wc1 <- wordcloud2(data = head(freq1, 200), color =  "random-dark", size = 0.4)
saveWidget(wc1, "./images/wc1.html", selfcontained = F)
webshot("./images/wc1.html", "wc1.png", delay = 2, vwidth = 650, vheight = 470)
# Word clouds for Bigram
wc2 <- wordcloud2(data = head(freq2, 100), color =  "random-dark", size = 0.3)
saveWidget(wc2, "./images/wc2.html", selfcontained = F)
webshot("./images/wc2.html", "wc2.png", delay = 2, vwidth = 650, vheight = 470)
# Word clouds for Trigram
wc3 <- wordcloud2(data = head(freq3, 100), color =  "random-dark", size = 0.3)
saveWidget(wc3, "./images/wc3.html", selfcontained = F)
webshot("./images/wc3.html", "wc3.png", delay = 2, vwidth = 650, vheight = 470)
```

### 4c. Clustering <a name="clustering"></a>

```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}

# Clustering for Unigram
# removeSparseTerms: to reduce the size of frequent words to around 20
tdm4 <- removeSparseTerms(tdm1, 0.965)
d <- dist(scale(tdm4), method="euclidian")   
fit <- hclust(d=d, method="complete") 
plot.new()
plot(fit, hang=-1, main ="Hierarchical clustering for Unigram",
     xlab = "term", ylab = "")

# Clustering for Bigram
tdm4 <- removeSparseTerms(tdm2, 0.9977) 
d <- dist(scale(tdm4), method="euclidian")   
fit <- hclust(d=d, method="complete") 
plot.new()
plot(fit, hang=-1, main ="Hierarchical clustering for Bigram",
     xlab = "terms", ylab = "")

# Clustering for Trigram
tdm4 <- removeSparseTerms(tdm3, 0.99976) 
d <- dist(scale(tdm4), method="euclidian")   
fit <- hclust(d=d, method="complete") 
plot.new()
plot(fit, hang=-1, main ="Hierarchical clustering for Trigram",
     xlab = "terms", ylab = "")
rm(tdm4, d, fit)
```


## 5. Findings <a name="findings"></a>

Interestingly, we found that Bigram and Trigram contain more information than Unigram. There were some common topics or phrases. N-gram could provide a glance at trending topics.

We also found that Twitter data included a lot of shorthand and improper English words. We had to reconsider if Twitter data was a trustworthy source for our ultimate goal: word prediction.

When reviewing our data, although we focused only on English, some special characters and non-English words might still exist. So, we decided to remove all non-English characters, and to perform stemming in our pre-precessing data.

Finally, processing data was quite time-consuming, especially when converted to a term-document matrix. Due to computational limitations, we were only able to process a sample of the original data that was needed. Therefore, we might have to look for a better dataset to save our resources in our next step.
    
## 6. Future plans <a name="plans"></a>

As explained in our background information, our ultimate goal is to develop a text prediction application based on our prediction algorithm. We will deploy this application on a Shiny server to make our Shiny application available over the web. 

To achieve this objective, we have to train an accurate prediction algorithm. The data analysis in the report provided us with an insight that words frequency and the combination might be a possible way for text prediction. However, this method required us to process an enormous amount of data.

Therefore, we may also consider Hidden Markov Models (HMM) for text prediction. A hidden Markov model is a statistical model that models sequential data. It is usual for speech recognition, part-of-speech tagging, gene prediction etc. However, this model requires a good dataset for training. 

For our application, we want to provide an interactive text prediction feature. Providing a list of texts as a prediction result would be a possible way. Users can inspect the probability of each text prediction, and pick the one they like.


## Appendix A. Top 100 frequent words in N-gram  <a name="appendixA"></a>
```{r echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Inspect Top 100 frequent words in Unigram
freq1$word[1:100]
# Inspect Top 100 frequent words in bigram
freq2$word[1:100]
# Inspect Top 100 frequent words in Trigram
freq3$word[1:100]
```

## Appendix B. Full Code in R <a name="appendixB"></a>
```{r echo = TRUE, eval=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Load libraries
require(tm); require(SnowballC); require(stopwords); 
require(ggplot2); require(tokenizers);  require(dplyr);
require(wordcloud2); require(webshot); require(htmlwidgets)

url_tw <- "./data/en_US.twitter.txt"
url_blog <- "./data/en_US.blogs.txt"
url_news <- "./data/en_US.news.txt"
conn_tw <- file(url_tw,open="r")
conn_blog <- file(url_blog,open="r")
conn_news <- file(url_news,open="r")

# Load data
lines_tw <-readLines(conn_tw)
lines_blog <-readLines(conn_blog)
lines_news <-readLines(conn_news)

line_counts <- c(length(lines_tw), length(lines_blog), length(lines_news))
word_counts <- c(sum(nchar(lines_tw)), sum(nchar(lines_blog)),sum(nchar(lines_blog)))
size_counts <- c(format(object.size(lines_tw), units = "Mb"),
                format(object.size(lines_blog), units = "Mb"),
                format(object.size(lines_news), units = "Mb"))

# Process basic infomation
summ <- data.frame('Lines_counts' = line_counts, 
                         'Word_counts' = word_counts, 
                         'File_size' = size_counts,
                      row.names = c('en_US.twitter', 'en_US.blogs', 'en_US.news'))
summ

close(conn_tw)
close(conn_blog)
close(conn_news)
rm(url_tw, url_blog, url_news, conn_blog, conn_news, conn_tw)

# create a list of random variables
set.seed(1234)
# Sample a part of each dataset
percent <- 0.01
lines_tw <- sample(lines_tw, summ$Lines_counts[1]*percent, replace=FALSE)
lines_blog <- sample(lines_blog, summ$Lines_counts[2]*percent, replace=FALSE)
lines_news <- sample(lines_news, summ$Lines_counts[3]*percent, replace=FALSE)

# 1. Text cleaning:
myCorpus <- VCorpus(VectorSource(c(lines_tw, lines_blog, lines_news)))
rm(lines_tw, lines_blog, lines_news)
removeURL <- function(x) {gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)}
removeSpecial <- function(x) {gsub("[^a-zA-Z ]+", "", x)} # except space
removeEmpty <- function(x){!is.na(content(x)) & trimws(content(x)) != ""}

# Preprocessing
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, content_transformer(removeSpecial))

myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))

# Remove documents with empty content to reduce size
myCorpus <- tm_filter(myCorpus, removeEmpty)

# Stemming
myCorpus <- tm_map(myCorpus, stemDocument)

rm(removeURL, removeSpecial)
print(paste0('Corpus size after cleaning: ', format(object.size(myCorpus), units = "Mb")))

# 2. N-gram
# create N-gram tern document matrix from a corpus
createNgram <- function(myCorpus, ngram = 1, lowfreq = NA) {
    toToken <- function(x) {
        # x contains x[1]$content and x[2]$meta
        unlist(tokenize_ngrams(content(x), n = ngram, n_min = ngram))
    }
    
    # TDM automatically discards words less than three characters. 
    tdm <- TermDocumentMatrix(myCorpus, control = list(tokenize = toToken,
                                                       wordLengths=c(1, Inf)))

    if (!is.na(lowfreq)) {
        # remove infrequent words to reduce size
        freqTerms <- findFreqTerms(tdm, lowfreq = lowfreq)
        tdm <- tdm[freqTerms, ]
    }
    tdm
}

convertTdmToDf <- function(tdm) {
    # Reduce documents dimension and convert to dataframe
    # df <- data.frame(rowSums(as.matrix(tdm))) # exhaust memory
    df <- slam::row_sums(tdm, na.rm = T)
    df <- data.frame(word = names(df), freq = as.integer(df))  %>%
        arrange(desc(freq))
    rm(tdm)
    print(paste0("Ngram size: ", format(object.size(df), units = "Mb")))
    df
}

# N-gram
# removeSparseTerms: remove sparse words to contain around 1000 frequent words

# Unigram
tdm1 <- createNgram(myCorpus, n = 1, lowfreq = 1)
freq1 <- convertTdmToDf(tdm1) # Create a frequency sorted dataframe for tokens
# Bigram
tdm2 <- createNgram(myCorpus, n = 2, lowfreq = 1)
freq2 <- convertTdmToDf(tdm2) # Create a frequency sorted dataframe for tokens
# Trigram 
tdm3 <- createNgram(myCorpus, n = 3, lowfreq = 1)
freq3 <- convertTdmToDf(tdm3) # Create a frequency sorted dataframe for tokens
rm(myCorpus)

# Plot Unigram
ggplot(head(freq1,20), aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat = "identity") +
  labs(x = "Terms",
           y = "Count") + 
      ggtitle("Top 20 frequent terms in Unigram") +
  coord_flip()

# Plot Bigram
ggplot(head(freq2,20), aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat = "identity") +
  labs(x = "Terms",
           y = "Count") + 
      ggtitle("Top 20 frequent terms in Bigram") +
  coord_flip()

# Plot Trigram
ggplot(head(freq3,20), aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat = "identity") +
  labs(x = "Terms",
           y = "Count") + 
      ggtitle("Top 20 frequent terms in Trigram") +
  coord_flip()

# Word clouds for Unigram
wc1 <- wordcloud2(data = head(freq1, 200), color =  "random-dark", size = 0.4)
saveWidget(wc1, "./images/wc1.html", selfcontained = F)
webshot("./images/wc1.html", "wc1.png", delay = 2, vwidth = 650, vheight = 470)
# Word clouds for Bigram
wc2 <- wordcloud2(data = head(freq2, 100), color =  "random-dark", size = 0.3)
saveWidget(wc2, "./images/wc2.html", selfcontained = F)
webshot("./images/wc2.html", "wc2.png", delay = 2, vwidth = 650, vheight = 470)
# Word clouds for Trigram
wc3 <- wordcloud2(data = head(freq3, 100), color =  "random-dark", size = 0.3)
saveWidget(wc3, "./images/wc3.html", selfcontained = F)
webshot("./images/wc3.html", "wc3.png", delay = 2, vwidth = 650, vheight = 470)

# Clustering for Unigram
# removeSparseTerms: to reduce the size of frequent words to around 20
tdm4 <- removeSparseTerms(tdm1, 0.965)
d <- dist(scale(tdm4), method="euclidian")   
fit <- hclust(d=d, method="complete") 
plot.new()
plot(fit, hang=-1, main ="Hierarchical clustering for Unigram",
     xlab = "term", ylab = "")

# Clustering for Bigram
tdm4 <- removeSparseTerms(tdm2, 0.9977) 
d <- dist(scale(tdm4), method="euclidian")   
fit <- hclust(d=d, method="complete") 
plot.new()
plot(fit, hang=-1, main ="Hierarchical clustering for Bigram",
     xlab = "terms", ylab = "")

# Clustering for Trigram
tdm4 <- removeSparseTerms(tdm3, 0.99976) 
d <- dist(scale(tdm4), method="euclidian")   
fit <- hclust(d=d, method="complete") 
plot.new()
plot(fit, hang=-1, main ="Hierarchical clustering for Trigram",
     xlab = "terms", ylab = "")
rm(tdm4, d, fit)
```
